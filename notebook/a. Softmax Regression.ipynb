{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243f6ce9-cdce-4c00-9bc0-d8450beaa4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64055dca-52e0-4ccb-9133-a5a15a9ab2bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trainning Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634fb101-d981-4ef3-b5ad-038dad225734",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 9\n",
    "outputs = 5\n",
    "data = 100\n",
    "\n",
    "m = data\n",
    "\n",
    "X = (np.random.random((inputs, data)) - 0.5) * 2\n",
    "ys = np.random.randint(0, outputs, size=data)\n",
    "Y = np.zeros((outputs, data))\n",
    "for i in range(data):\n",
    "    Y[ys[i]][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72cdd81-903e-455f-bc12-0c279bd56191",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd7c10-06df-4297-beee-d8366d864463",
   "metadata": {},
   "source": [
    "### hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f60417-ac85-4118-b0e3-6090ef2fac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000\n",
    "alpha = 0.01\n",
    "decay_rate = 1\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "lambd = 0.5\n",
    "batch = 64 # batch size, power of 2\n",
    "layers = [None, 5, 5, 5, 5, outputs]\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e3c77-336e-46ef-998a-ad7fc60bf953",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9b6a2-c866-4c3b-9c2a-30111f374d94",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05343473-6575-46cc-9de6-dd4cd9705838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)\n",
    "def d_softmax(z):\n",
    "    summ = np.sum(np.exp(z), axis=0, keepdims=True)\n",
    "    return (np.exp(z) * summ - np.exp(2*z)) / (summ ** 2)\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "def d_ReLU(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))\n",
    "\n",
    "def tanh(z):\n",
    "    # x = (np.exp(z) + np.exp(-z))/(np.exp(z) - np.exp(-z))\n",
    "    return np.tanh(z)\n",
    "def d_tanh(z):\n",
    "    return 1 - tanh(z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbbd192-72b2-4488-b8b5-3e5e856b0a71",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f7eb3f-4b4e-4df5-be4c-333a7ec5e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y_hat, Y):\n",
    "    return -np.sum(Y * np.log(Y_hat + epsilon)) / Y.shape[1]\n",
    "def d_cost(Y_hat, Y):\n",
    "    return -(Y / (Y_hat + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143332c9-06d8-4420-9081-fe13d92bb397",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84abc0a1-738b-4144-9bc7-836fe3f9b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract mean\n",
    "mu = np.sum(X) / m\n",
    "X = X - mu\n",
    "\n",
    "# Normalize Variance\n",
    "sigma = np.sum(X**2)/m\n",
    "X /= sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5dc77-db67-42d3-adc1-3ef7f14a8cb2",
   "metadata": {},
   "source": [
    "### Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bcd5ae-db55-4f6c-aa90-1db4cffb43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[0] = inputs\n",
    "layer = len(layers) - 1\n",
    "batch_num = int(m / batch + 1)\n",
    "X = np.array_split(X, batch_num, 1)\n",
    "Y = np.array_split(Y, batch_num, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37a1a3-6a39-40ab-9869-0fbc6c0730d5",
   "metadata": {},
   "source": [
    "### Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e8ed5b-d95f-41ad-a0c4-1d7e0d115fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [X] + [None] * layer\n",
    "Z = [None] + [None] * layer\n",
    "g = [None] + [ReLU] * (layer - 1) + [sigmoid]\n",
    "dg = [None] + [d_ReLU] * (layer - 1) + [d_sigmoid]\n",
    "W = [None]\n",
    "VdW = [None]\n",
    "SdW = [None]\n",
    "Beta = [None]\n",
    "VdBeta = [None]\n",
    "SdBeta = [None]\n",
    "Gama = [None]\n",
    "VdGama = [None]\n",
    "SdGama = [None]\n",
    "rate = alpha\n",
    "epoch_num = 0\n",
    "\n",
    "# Weight Initialization\n",
    "for l in range(1, layer + 1):\n",
    "    # weight = weight * Var(W)\n",
    "    # ReLU: Var(W) = sqrt(2 / n[l-1])\n",
    "    # tanh: Var(W) = sqrt(1 / n[l-1]) \n",
    "    #             or sqrt(2 / (n[l-1] + n[l]))\n",
    "    W.append(np.random.random((layers[l], layers[l-1]))*np.sqrt(2/layers[l-1]))\n",
    "    VdW.append(np.zeros((layers[l], layers[l-1])))\n",
    "    SdW.append(np.zeros((layers[l], layers[l-1])))\n",
    "    \n",
    "    Beta.append(np.zeros((layers[l], 1)))\n",
    "    VdBeta.append(np.zeros((layers[l], 1)))\n",
    "    SdBeta.append(np.zeros((layers[l], 1)))\n",
    "    \n",
    "    Gama.append(np.zeros((layers[l], 1)))\n",
    "    VdGama.append(np.zeros((layers[l], 1)))\n",
    "    SdGama.append(np.zeros((layers[l], 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcc6cf-1244-4c7e-8adf-d5ba1df38a82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737a7172-8cd0-4f26-8f18-858c7d1bbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20%\n",
      "loss: 0.03397828413380877\n",
      "\n",
      "40%\n",
      "loss: 0.03215774682424253\n",
      "\n",
      "60%\n",
      "loss: 0.031133092783166956\n",
      "\n",
      "80%\n",
      "loss: 0.030422720939252412\n",
      "\n",
      "100%\n",
      "loss: 0.029880640347489645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, epoch + 1):\n",
    "    L = 0\n",
    "    # learning rate decay\n",
    "    rate = 1 / (1 + decay_rate * epoch_num) * alpha\n",
    "    for t in range(batch_num):\n",
    "        A[0] = X[t]\n",
    "        for l in range(1, layer + 1):\n",
    "            Z[l] = np.dot(W[l], A[l - 1])\n",
    "            mu = np.sum(Z[l]) / m\n",
    "            sigma = np.sum((Z[l] - mu)**2) / m\n",
    "            Z_norm = (Z[l] - mu) / (sigma + epsilon) ** (0.5)\n",
    "            Z_tilde = Gama[l] * Z_norm + Beta[l]\n",
    "            A[l] = g[l](Z_tilde)\n",
    "        Y_hat = A[layer]\n",
    "        L += cost(Y_hat, Y[t])\n",
    "\n",
    "        dA = d_cost(Y_hat, Y[t])\n",
    "        for l in range(layer, 0, -1):\n",
    "            # recalculate constants\n",
    "            mu = np.sum(Z[l]) / m\n",
    "            sigma = np.sum((Z[l] - mu)**2) / m\n",
    "            Z_norm = (Z[l] - mu) / (sigma + epsilon) ** (0.5)\n",
    "            Z_tilde = Gama[l] * Z_norm + Beta[l]\n",
    "            \n",
    "            # Calculating derivative\n",
    "            dZ_tilde = dA * dg[l](Z_tilde)\n",
    "            dZ_norm = dZ_tilde * Gama[l]\n",
    "            dZ = dZ_norm * (sigma + epsilon) ** (-0.5)\n",
    "            dW = np.dot(dZ, A[l-1].T) / m\n",
    "            dGama = np.sum(dZ_tilde * Z_norm, axis=1, keepdims=True) / m\n",
    "            dBeta = np.sum(dZ_tilde, axis=1, keepdims=True) / m\n",
    "            \n",
    "            # Regularized derivative\n",
    "            dW += lambd / m * W[l]\n",
    "            dGama += lambd / m * Gama[l]\n",
    "            \n",
    "            dA = np.dot(W[l].T, dZ)\n",
    "            \n",
    "            # Momentum & RMSprop\n",
    "            VdW[l] = beta_1 * VdW[l] + (1-beta_1)*dW\n",
    "            VdBeta[l] = beta_1 * VdBeta[l] + (1-beta_1)*dBeta\n",
    "            VdGama[l] = beta_1 * VdGama[l] + (1-beta_1)*dGama\n",
    "            SdW[l] = beta_2 * SdW[l] + (1-beta_2)*dW**2\n",
    "            SdBeta[l] = beta_2 * SdBeta[l] + (1-beta_2)*dBeta**2\n",
    "            SdGama[l] = beta_2 * SdGama[l] + (1-beta_2)*dGama**2\n",
    "            \n",
    "            # Required: Bias correction\n",
    "            VdW_corrected = VdW[l] / (1 - beta_1**(epoch_num*batch_num+t+1))\n",
    "            VdBeta_corrected = VdBeta[l] / (1 - beta_1**(epoch_num*batch_num+t+1))\n",
    "            VdGama_corrected = VdGama[l] / (1 - beta_1**(epoch_num*batch_num+t+1))\n",
    "            SdW_corrected = SdW[l] / (1 - beta_2**(epoch_num*batch_num+t+1))\n",
    "            SdBeta_corrected = SdBeta[l] / (1 - beta_2**(epoch_num*batch_num+t+1))\n",
    "            SdGama_corrected = SdGama[l] / (1 - beta_2**(epoch_num*batch_num+t+1))\n",
    "            \n",
    "            # Gradient descent\n",
    "            W[l] = W[l] - VdW_corrected / (SdW_corrected**(1/2) + epsilon) * rate\n",
    "            Gama[l] = Gama[l] - VdGama_corrected / (SdGama_corrected**(1/2) + epsilon) * rate\n",
    "            Beta[l] = Beta[l] - VdBeta_corrected / (SdBeta_corrected**(1/2) + epsilon) * rate\n",
    "    \n",
    "    epoch_num += 1\n",
    "    \n",
    "    # Regularized cost\n",
    "    J = L / m\n",
    "    for l in range(1, layer + 1):\n",
    "        J += lambd / (2*m) * np.sum(W[l]**2)\n",
    "    \n",
    "    if((int(i / epoch * 100)) % 20 == 0 and int(i * 100 / epoch) == i * 100 / epoch):\n",
    "        sys.stdout.write(str(int(i / epoch * 100)) + \"%\\nloss: \" + str(J) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2d41e-5015-4435-8ad0-95af5940679e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
